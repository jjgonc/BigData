{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44bd1715-889a-4e4e-99fa-cc88f009bd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1b2228-ad9a-4e48-8925-2e986190e0ca",
   "metadata": {},
   "source": [
    "## Carregar os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "724437d3-0c7f-43c5-8707-a039d9a99887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar o dataset JSON\n",
    "with open('../datasets/HPI_master.json', 'r') as f:\n",
    "    json_data = json.load(f)\n",
    "\n",
    "# Converter o dataset JSON para um DataFrame do pandas\n",
    "data_json = pd.DataFrame(json_data)\n",
    "\n",
    "# Carregar o dataset CSV - Population size\n",
    "data_csv_ps = pd.read_csv(\"../datasets/cu.data.19.PopulationSize.csv\")\n",
    "data_csv_ps = pd.DataFrame(data_csv_ps)\n",
    "\n",
    "# Carregar o dataset CSV - \n",
    "data_csv_fb = pd.read_csv(\"../datasets/cu.data.11.USFoodBeverage.csv\")\n",
    "data_csv_fb = pd.DataFrame(data_csv_fb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21cdd734-e3ff-4b52-96dd-943191e78667",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Visualizar os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a881d113-6f6d-4d0b-8470-eacba8626d38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hpi_type</th>\n",
       "      <th>hpi_flavor</th>\n",
       "      <th>frequency</th>\n",
       "      <th>level</th>\n",
       "      <th>place_name</th>\n",
       "      <th>place_id</th>\n",
       "      <th>yr</th>\n",
       "      <th>period</th>\n",
       "      <th>index_nsa</th>\n",
       "      <th>index_sa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>traditional</td>\n",
       "      <td>purchase-only</td>\n",
       "      <td>monthly</td>\n",
       "      <td>USA or Census Division</td>\n",
       "      <td>East North Central Division</td>\n",
       "      <td>DV_ENC</td>\n",
       "      <td>1991</td>\n",
       "      <td>1</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>traditional</td>\n",
       "      <td>purchase-only</td>\n",
       "      <td>monthly</td>\n",
       "      <td>USA or Census Division</td>\n",
       "      <td>East North Central Division</td>\n",
       "      <td>DV_ENC</td>\n",
       "      <td>1991</td>\n",
       "      <td>2</td>\n",
       "      <td>100.91</td>\n",
       "      <td>100.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>traditional</td>\n",
       "      <td>purchase-only</td>\n",
       "      <td>monthly</td>\n",
       "      <td>USA or Census Division</td>\n",
       "      <td>East North Central Division</td>\n",
       "      <td>DV_ENC</td>\n",
       "      <td>1991</td>\n",
       "      <td>3</td>\n",
       "      <td>101.30</td>\n",
       "      <td>100.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>traditional</td>\n",
       "      <td>purchase-only</td>\n",
       "      <td>monthly</td>\n",
       "      <td>USA or Census Division</td>\n",
       "      <td>East North Central Division</td>\n",
       "      <td>DV_ENC</td>\n",
       "      <td>1991</td>\n",
       "      <td>4</td>\n",
       "      <td>101.69</td>\n",
       "      <td>100.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>traditional</td>\n",
       "      <td>purchase-only</td>\n",
       "      <td>monthly</td>\n",
       "      <td>USA or Census Division</td>\n",
       "      <td>East North Central Division</td>\n",
       "      <td>DV_ENC</td>\n",
       "      <td>1991</td>\n",
       "      <td>5</td>\n",
       "      <td>102.32</td>\n",
       "      <td>101.36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      hpi_type     hpi_flavor frequency                   level  \\\n",
       "0  traditional  purchase-only   monthly  USA or Census Division   \n",
       "1  traditional  purchase-only   monthly  USA or Census Division   \n",
       "2  traditional  purchase-only   monthly  USA or Census Division   \n",
       "3  traditional  purchase-only   monthly  USA or Census Division   \n",
       "4  traditional  purchase-only   monthly  USA or Census Division   \n",
       "\n",
       "                    place_name place_id    yr  period  index_nsa index_sa  \n",
       "0  East North Central Division   DV_ENC  1991       1     100.00    100.0  \n",
       "1  East North Central Division   DV_ENC  1991       2     100.91   100.96  \n",
       "2  East North Central Division   DV_ENC  1991       3     101.30   100.91  \n",
       "3  East North Central Division   DV_ENC  1991       4     101.69   100.98  \n",
       "4  East North Central Division   DV_ENC  1991       5     102.32   101.36  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_json.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25764e50-b56a-499c-89d5-de5489bb229a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>series_id</th>\n",
       "      <th>year</th>\n",
       "      <th>period</th>\n",
       "      <th>value</th>\n",
       "      <th>footnote_codes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CUURA000AA0</td>\n",
       "      <td>1986</td>\n",
       "      <td>M12</td>\n",
       "      <td>100.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CUURA000AA0</td>\n",
       "      <td>1987</td>\n",
       "      <td>M01</td>\n",
       "      <td>100.6</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CUURA000AA0</td>\n",
       "      <td>1987</td>\n",
       "      <td>M02</td>\n",
       "      <td>101.1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CUURA000AA0</td>\n",
       "      <td>1987</td>\n",
       "      <td>M03</td>\n",
       "      <td>101.6</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CUURA000AA0</td>\n",
       "      <td>1987</td>\n",
       "      <td>M04</td>\n",
       "      <td>102.2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     series_id  year period  value  footnote_codes\n",
       "0  CUURA000AA0  1986    M12  100.0             NaN\n",
       "1  CUURA000AA0  1987    M01  100.6             NaN\n",
       "2  CUURA000AA0  1987    M02  101.1             NaN\n",
       "3  CUURA000AA0  1987    M03  101.6             NaN\n",
       "4  CUURA000AA0  1987    M04  102.2             NaN"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_csv_ps.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "814080de-f4e2-4e2c-ac62-191d600577fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>series_id</th>\n",
       "      <th>year</th>\n",
       "      <th>period</th>\n",
       "      <th>value</th>\n",
       "      <th>footnote_codes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CUSR0000SAF</td>\n",
       "      <td>1967</td>\n",
       "      <td>M01</td>\n",
       "      <td>34.8</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CUSR0000SAF</td>\n",
       "      <td>1967</td>\n",
       "      <td>M02</td>\n",
       "      <td>34.7</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CUSR0000SAF</td>\n",
       "      <td>1967</td>\n",
       "      <td>M03</td>\n",
       "      <td>34.7</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CUSR0000SAF</td>\n",
       "      <td>1967</td>\n",
       "      <td>M04</td>\n",
       "      <td>34.6</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CUSR0000SAF</td>\n",
       "      <td>1967</td>\n",
       "      <td>M05</td>\n",
       "      <td>34.6</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     series_id  year period  value  footnote_codes\n",
       "0  CUSR0000SAF  1967    M01   34.8             NaN\n",
       "1  CUSR0000SAF  1967    M02   34.7             NaN\n",
       "2  CUSR0000SAF  1967    M03   34.7             NaN\n",
       "3  CUSR0000SAF  1967    M04   34.6             NaN\n",
       "4  CUSR0000SAF  1967    M05   34.6             NaN"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_csv_fb.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7be446a5",
   "metadata": {},
   "source": [
    "#### Tipos de dados do dataset: Food & Beverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c43e660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 119198 entries, 0 to 119197\n",
      "Data columns (total 5 columns):\n",
      " #   Column          Non-Null Count   Dtype  \n",
      "---  ------          --------------   -----  \n",
      " 0   series_id       119198 non-null  object \n",
      " 1   year            119198 non-null  int64  \n",
      " 2   period          119198 non-null  object \n",
      " 3   value           119198 non-null  float64\n",
      " 4   footnote_codes  0 non-null       float64\n",
      "dtypes: float64(2), int64(1), object(2)\n",
      "memory usage: 4.5+ MB\n"
     ]
    }
   ],
   "source": [
    "data_csv_fb.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684e7388-9e68-4a83-9250-4c7ec5b64b8c",
   "metadata": {},
   "source": [
    "#### **Tipos de dados do dataset: Population Size**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2a5708e-7465-4e2f-89c5-60fb1d52c64f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 84760 entries, 0 to 84759\n",
      "Data columns (total 5 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   series_id       84760 non-null  object \n",
      " 1   year            84760 non-null  int64  \n",
      " 2   period          84760 non-null  object \n",
      " 3   value           84760 non-null  float64\n",
      " 4   footnote_codes  0 non-null      float64\n",
      "dtypes: float64(2), int64(1), object(2)\n",
      "memory usage: 3.2+ MB\n"
     ]
    }
   ],
   "source": [
    "data_csv_ps.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45295a4-a374-4d00-9394-8f52546cd167",
   "metadata": {},
   "source": [
    "Conseguimos observar a existência de dados do tipo `object`. Poderá ser necessário tratar este tipo de dados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365eb01d-ba04-4fe3-9d87-f0c9352110e9",
   "metadata": {},
   "source": [
    "#### **Tipos de dados do dataset Json**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5dceddf7-6782-4613-a6d3-371c0ae0f69c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hpi_type       object\n",
       "hpi_flavor     object\n",
       "frequency      object\n",
       "level          object\n",
       "place_name     object\n",
       "place_id       object\n",
       "yr              int64\n",
       "period          int64\n",
       "index_nsa     float64\n",
       "index_sa       object\n",
       "dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_json.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16a2b15-7462-4349-a2d3-0d68369901da",
   "metadata": {},
   "source": [
    "#### **Verificar os valores da coluna `period`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8599a5c-94a3-4e54-8770-0398956ec23b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 'period' column contains the following values:\n",
      "['M12' 'M01' 'M02' 'M03' 'M04' 'M05' 'M06' 'M07' 'M08' 'M09' 'M10' 'M11'\n",
      " 'M13' 'S01' 'S02' 'S03']\n"
     ]
    }
   ],
   "source": [
    "# List all the different possible values in the 'period' column\n",
    "period_values = data_csv_ps['period'].unique()\n",
    "print(\"The 'period' column contains the following values:\")\n",
    "print(period_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627fa9cc-b961-45d6-9742-2039fb3ee8b2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Tratamento dos Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964749c0-56f4-403d-8863-9f998d8fec03",
   "metadata": {},
   "source": [
    "### **Transformar `index_sa` em dados numéricos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f89bc6b-623d-44c6-84e2-1c1703e073f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_json['index_sa'] = data_json['index_sa'].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5c3656-876d-41a4-a29e-00fd05c71365",
   "metadata": {},
   "source": [
    "#### **Transformar `period` em dados numéricos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "199ef4f2-f8e0-447c-99e4-57a0e715aa50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         1\n",
       "1         2\n",
       "2         3\n",
       "3         4\n",
       "4         5\n",
       "         ..\n",
       "121457    4\n",
       "121458    1\n",
       "121459    2\n",
       "121460    3\n",
       "121461    4\n",
       "Name: period, Length: 121462, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dicionário para mapear o período número ao valor da string\n",
    "period_map = {\n",
    "    'M01': 1,\n",
    "    'M02': 2,\n",
    "    'M03': 3,\n",
    "    'M04': 4,\n",
    "    'M05': 5,\n",
    "    'M06': 6,\n",
    "    'M07': 7,\n",
    "    'M08': 8,\n",
    "    'M09': 9,\n",
    "    'M10': 10,\n",
    "    'M11': 11,\n",
    "    'M12': 12,\n",
    "    'M13': 13,\n",
    "    'S01': 14,\n",
    "    'S02': 15,\n",
    "    'S03': 16\n",
    "}\n",
    "\n",
    "data_csv_ps['period'] = data_csv_ps[\"period\"].replace(period_map)\n",
    "data_csv_fb['period'] = data_csv_fb[\"period\"].replace(period_map)\n",
    "data_json['period'].astype(str).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c7459a-e86b-43a0-bc77-be163af43316",
   "metadata": {},
   "source": [
    "#### **Remover a coluna `footnote_codes`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "957b5975-452b-4d27-b68f-198d9aee9071",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_csv_ps.drop(columns=[\"footnote_codes\"], inplace=True)\n",
    "data_csv_fb.drop(columns=[\"footnote_codes\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "41f042f6-bab8-427d-b856-83554b2efbdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>series_id</th>\n",
       "      <th>year</th>\n",
       "      <th>period</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CUURA000AA0</td>\n",
       "      <td>1986</td>\n",
       "      <td>12</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CUURA000AA0</td>\n",
       "      <td>1987</td>\n",
       "      <td>1</td>\n",
       "      <td>100.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CUURA000AA0</td>\n",
       "      <td>1987</td>\n",
       "      <td>2</td>\n",
       "      <td>101.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CUURA000AA0</td>\n",
       "      <td>1987</td>\n",
       "      <td>3</td>\n",
       "      <td>101.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CUURA000AA0</td>\n",
       "      <td>1987</td>\n",
       "      <td>4</td>\n",
       "      <td>102.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     series_id  year  period  value\n",
       "0  CUURA000AA0  1986      12  100.0\n",
       "1  CUURA000AA0  1987       1  100.6\n",
       "2  CUURA000AA0  1987       2  101.1\n",
       "3  CUURA000AA0  1987       3  101.6\n",
       "4  CUURA000AA0  1987       4  102.2"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_csv_ps.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1e49c821-24e5-4c97-af55-fbe33f4430d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>series_id</th>\n",
       "      <th>year</th>\n",
       "      <th>period</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CUSR0000SAF</td>\n",
       "      <td>1967</td>\n",
       "      <td>1</td>\n",
       "      <td>34.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CUSR0000SAF</td>\n",
       "      <td>1967</td>\n",
       "      <td>2</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CUSR0000SAF</td>\n",
       "      <td>1967</td>\n",
       "      <td>3</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CUSR0000SAF</td>\n",
       "      <td>1967</td>\n",
       "      <td>4</td>\n",
       "      <td>34.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CUSR0000SAF</td>\n",
       "      <td>1967</td>\n",
       "      <td>5</td>\n",
       "      <td>34.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     series_id  year  period  value\n",
       "0  CUSR0000SAF  1967       1   34.8\n",
       "1  CUSR0000SAF  1967       2   34.7\n",
       "2  CUSR0000SAF  1967       3   34.7\n",
       "3  CUSR0000SAF  1967       4   34.6\n",
       "4  CUSR0000SAF  1967       5   34.6"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_csv_fb.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d3b6208f-8ea5-4adf-a8af-8059c6d4f820",
   "metadata": {},
   "source": [
    "#### Tratar Nan Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cb6ed618-ed38-4d1e-b568-d9c547ee3799",
   "metadata": {},
   "outputs": [],
   "source": [
    "median_value = data_json['index_sa'].median()\n",
    "data_json['index_sa'] = data_json['index_sa'].fillna(median_value)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0045c4a4",
   "metadata": {},
   "source": [
    "#### Renomear colunas"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "47595164",
   "metadata": {},
   "source": [
    "Estas colunas são necessárias renomear porque irão dar erro ao efetuar o merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "16a6a211",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_json.rename(columns = {'yr':'year'}, inplace = True)\n",
    "data_csv_fb.rename(columns = {'value':'valueFoodBeverage'}, inplace = True)\n",
    "data_csv_ps.rename(columns = {'value':'valuePopSize'}, inplace = True)\n",
    "data_csv_fb.rename(columns = {'series_id':'idFoodBeverage'}, inplace = True)\n",
    "data_csv_fb.rename(columns = {'series_id':'idPopSize'}, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1843ef-ffd5-43be-9dba-e5b245f46a27",
   "metadata": {},
   "source": [
    "## Converter para Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aa22fa1f-7eef-4e81-a810-6fb64a310c9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hpi_type</th>\n",
       "      <th>hpi_flavor</th>\n",
       "      <th>frequency</th>\n",
       "      <th>level</th>\n",
       "      <th>place_name</th>\n",
       "      <th>place_id</th>\n",
       "      <th>year</th>\n",
       "      <th>period</th>\n",
       "      <th>index_nsa</th>\n",
       "      <th>index_sa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>traditional</td>\n",
       "      <td>purchase-only</td>\n",
       "      <td>monthly</td>\n",
       "      <td>USA or Census Division</td>\n",
       "      <td>East North Central Division</td>\n",
       "      <td>DV_ENC</td>\n",
       "      <td>1991</td>\n",
       "      <td>1</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>traditional</td>\n",
       "      <td>purchase-only</td>\n",
       "      <td>monthly</td>\n",
       "      <td>USA or Census Division</td>\n",
       "      <td>East North Central Division</td>\n",
       "      <td>DV_ENC</td>\n",
       "      <td>1991</td>\n",
       "      <td>2</td>\n",
       "      <td>100.91</td>\n",
       "      <td>100.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>traditional</td>\n",
       "      <td>purchase-only</td>\n",
       "      <td>monthly</td>\n",
       "      <td>USA or Census Division</td>\n",
       "      <td>East North Central Division</td>\n",
       "      <td>DV_ENC</td>\n",
       "      <td>1991</td>\n",
       "      <td>3</td>\n",
       "      <td>101.30</td>\n",
       "      <td>100.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>traditional</td>\n",
       "      <td>purchase-only</td>\n",
       "      <td>monthly</td>\n",
       "      <td>USA or Census Division</td>\n",
       "      <td>East North Central Division</td>\n",
       "      <td>DV_ENC</td>\n",
       "      <td>1991</td>\n",
       "      <td>4</td>\n",
       "      <td>101.69</td>\n",
       "      <td>100.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>traditional</td>\n",
       "      <td>purchase-only</td>\n",
       "      <td>monthly</td>\n",
       "      <td>USA or Census Division</td>\n",
       "      <td>East North Central Division</td>\n",
       "      <td>DV_ENC</td>\n",
       "      <td>1991</td>\n",
       "      <td>5</td>\n",
       "      <td>102.32</td>\n",
       "      <td>101.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121457</th>\n",
       "      <td>developmental</td>\n",
       "      <td>purchase-only</td>\n",
       "      <td>quarterly</td>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>PR</td>\n",
       "      <td>2021</td>\n",
       "      <td>4</td>\n",
       "      <td>185.03</td>\n",
       "      <td>183.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121458</th>\n",
       "      <td>developmental</td>\n",
       "      <td>purchase-only</td>\n",
       "      <td>quarterly</td>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>PR</td>\n",
       "      <td>2022</td>\n",
       "      <td>1</td>\n",
       "      <td>185.82</td>\n",
       "      <td>190.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121459</th>\n",
       "      <td>developmental</td>\n",
       "      <td>purchase-only</td>\n",
       "      <td>quarterly</td>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>PR</td>\n",
       "      <td>2022</td>\n",
       "      <td>2</td>\n",
       "      <td>179.30</td>\n",
       "      <td>179.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121460</th>\n",
       "      <td>developmental</td>\n",
       "      <td>purchase-only</td>\n",
       "      <td>quarterly</td>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>PR</td>\n",
       "      <td>2022</td>\n",
       "      <td>3</td>\n",
       "      <td>190.09</td>\n",
       "      <td>187.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121461</th>\n",
       "      <td>developmental</td>\n",
       "      <td>purchase-only</td>\n",
       "      <td>quarterly</td>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>PR</td>\n",
       "      <td>2022</td>\n",
       "      <td>4</td>\n",
       "      <td>199.98</td>\n",
       "      <td>198.91</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>121462 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             hpi_type     hpi_flavor  frequency                   level  \\\n",
       "0         traditional  purchase-only    monthly  USA or Census Division   \n",
       "1         traditional  purchase-only    monthly  USA or Census Division   \n",
       "2         traditional  purchase-only    monthly  USA or Census Division   \n",
       "3         traditional  purchase-only    monthly  USA or Census Division   \n",
       "4         traditional  purchase-only    monthly  USA or Census Division   \n",
       "...               ...            ...        ...                     ...   \n",
       "121457  developmental  purchase-only  quarterly             Puerto Rico   \n",
       "121458  developmental  purchase-only  quarterly             Puerto Rico   \n",
       "121459  developmental  purchase-only  quarterly             Puerto Rico   \n",
       "121460  developmental  purchase-only  quarterly             Puerto Rico   \n",
       "121461  developmental  purchase-only  quarterly             Puerto Rico   \n",
       "\n",
       "                         place_name place_id  year  period  index_nsa  \\\n",
       "0       East North Central Division   DV_ENC  1991       1     100.00   \n",
       "1       East North Central Division   DV_ENC  1991       2     100.91   \n",
       "2       East North Central Division   DV_ENC  1991       3     101.30   \n",
       "3       East North Central Division   DV_ENC  1991       4     101.69   \n",
       "4       East North Central Division   DV_ENC  1991       5     102.32   \n",
       "...                             ...      ...   ...     ...        ...   \n",
       "121457                  Puerto Rico       PR  2021       4     185.03   \n",
       "121458                  Puerto Rico       PR  2022       1     185.82   \n",
       "121459                  Puerto Rico       PR  2022       2     179.30   \n",
       "121460                  Puerto Rico       PR  2022       3     190.09   \n",
       "121461                  Puerto Rico       PR  2022       4     199.98   \n",
       "\n",
       "        index_sa  \n",
       "0         100.00  \n",
       "1         100.96  \n",
       "2         100.91  \n",
       "3         100.98  \n",
       "4         101.36  \n",
       "...          ...  \n",
       "121457    183.17  \n",
       "121458    190.35  \n",
       "121459    179.96  \n",
       "121460    187.85  \n",
       "121461    198.91  \n",
       "\n",
       "[121462 rows x 10 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pq1 = data_csv_fb.to_parquet('../parquetFiles/data_fb.parquet')\n",
    "pq2 = data_csv_ps.to_parquet('../parquetFiles/data_ps.parquet')\n",
    "pq3 = data_json.to_parquet('../parquetFiles/data_json.parquet')\n",
    "\n",
    "pd.read_parquet('../parquetFiles/data_fb.parquet')\n",
    "pd.read_parquet('../parquetFiles/data_ps.parquet')\n",
    "pd.read_parquet('../parquetFiles/data_json.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814f2464-dda7-4921-aaf3-98366a612c9c",
   "metadata": {},
   "source": [
    "## Realizar o Merge dos datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0a3affdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/13 23:39:46 WARN Utils: Your hostname, josejoao-S540 resolves to a loopback address: 127.0.1.1; using 192.168.1.76 instead (on interface wlp0s20f3)\n",
      "23/04/13 23:39:46 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      ":: loading settings :: url = jar:file:/home/josejoao/anaconda3/envs/DAA/lib/python3.7/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/josejoao/.ivy2/cache\n",
      "The jars for the packages stored in: /home/josejoao/.ivy2/jars\n",
      "org.mongodb.spark#mongo-spark-connector_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-591a0709-e18e-48b7-85d7-da575ac24b1e;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.mongodb.spark#mongo-spark-connector_2.12;2.4.0 in central\n",
      "\tfound org.mongodb#mongo-java-driver;3.9.0 in central\n",
      "downloading https://repo1.maven.org/maven2/org/mongodb/spark/mongo-spark-connector_2.12/2.4.0/mongo-spark-connector_2.12-2.4.0.jar ...\n",
      "\t[SUCCESSFUL ] org.mongodb.spark#mongo-spark-connector_2.12;2.4.0!mongo-spark-connector_2.12.jar (177ms)\n",
      "downloading https://repo1.maven.org/maven2/org/mongodb/mongo-java-driver/3.9.0/mongo-java-driver-3.9.0.jar ...\n",
      "\t[SUCCESSFUL ] org.mongodb#mongo-java-driver;3.9.0!mongo-java-driver.jar (485ms)\n",
      ":: resolution report :: resolve 1337ms :: artifacts dl 676ms\n",
      "\t:: modules in use:\n",
      "\torg.mongodb#mongo-java-driver;3.9.0 from central in [default]\n",
      "\torg.mongodb.spark#mongo-spark-connector_2.12;2.4.0 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   2   |   2   |   2   |   0   ||   2   |   2   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-591a0709-e18e-48b7-85d7-da575ac24b1e\n",
      "\tconfs: [default]\n",
      "\t2 artifacts copied, 0 already retrieved (2543kB/24ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/13 23:39:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Row(period=1, year=1978, idFoodBeverage='CUSR0000SAF', valueFoodBeverage=68.1, series_id='CUURD000SETB01', valuePopSize=51.2, hpi_type='traditional', hpi_flavor='all-transactions', frequency='quarterly', level='USA or Census Division', place_name='United States', place_id='USA', index_nsa=79.58, index_sa=180.51)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from pyspark.sql.functions import *\n",
    "# from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as Func\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import expr\n",
    "from pyspark.sql.types import *\n",
    "# spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Ejemplo de conexión a MongoDB Atlas\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:2.4.0\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "pq1 = spark.read.format(\"parquet\").load('../parquetFiles/data_fb.parquet')\n",
    "pq2 = spark.read.format(\"parquet\").load('../parquetFiles/data_ps.parquet')\n",
    "pq3 = spark.read.format(\"parquet\").load('../parquetFiles/data_json.parquet')\n",
    "\n",
    "df4 = pq1.join(pq2, on=['period', 'year'], how='inner').join(pq3, on=['period', 'year'], how='inner')\n",
    "\n",
    "df4.head()\n",
    "\n",
    "# pq_merged = df4.to_parquet('../parquetFiles/merged.parquet')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e465acd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pq_merged = df4.write.parquet('../parquetFiles/merged.parquet')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "acaffa62",
   "metadata": {},
   "source": [
    "----\n",
    "## Store data on MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ae18cf22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/14 01:07:54 ERROR Executor: Exception in task 0.0 in stage 8.0 (TID 8)\n",
      "com.mongodb.MongoCommandException: Command failed with error 8000 (AtlasError): 'you are over your space quota, using 514 MB of 512 MB' on server ac-uueoxhj-shard-00-01.l1dei5j.mongodb.net:27017. The full response is { \"ok\" : 0, \"errmsg\" : \"you are over your space quota, using 514 MB of 512 MB\", \"code\" : 8000, \"codeName\" : \"AtlasError\" }\n",
      "\tat com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:179)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:293)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:255)\n",
      "\tat com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:99)\n",
      "\tat com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:444)\n",
      "\tat com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:72)\n",
      "\tat com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:200)\n",
      "\tat com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:269)\n",
      "\tat com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:131)\n",
      "\tat com.mongodb.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:419)\n",
      "\tat com.mongodb.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:257)\n",
      "\tat com.mongodb.operation.MixedBulkWriteOperation.access$700(MixedBulkWriteOperation.java:68)\n",
      "\tat com.mongodb.operation.MixedBulkWriteOperation$1.call(MixedBulkWriteOperation.java:201)\n",
      "\tat com.mongodb.operation.MixedBulkWriteOperation$1.call(MixedBulkWriteOperation.java:192)\n",
      "\tat com.mongodb.operation.OperationHelper.withReleasableConnection(OperationHelper.java:424)\n",
      "\tat com.mongodb.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:192)\n",
      "\tat com.mongodb.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:67)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:193)\n",
      "\tat com.mongodb.client.internal.MongoCollectionImpl.executeInsertMany(MongoCollectionImpl.java:520)\n",
      "\tat com.mongodb.client.internal.MongoCollectionImpl.insertMany(MongoCollectionImpl.java:504)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:121)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:119)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:119)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:118)\n",
      "\tat com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)\n",
      "\tat com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)\n",
      "\tat com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)\n",
      "\tat com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)\n",
      "\tat com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$1(MongoSpark.scala:118)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$1$adapted(MongoSpark.scala:117)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1011)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1011)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2278)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/04/14 01:07:54 WARN TaskSetManager: Lost task 0.0 in stage 8.0 (TID 8) (josejoao-S540.Home executor driver): com.mongodb.MongoCommandException: Command failed with error 8000 (AtlasError): 'you are over your space quota, using 514 MB of 512 MB' on server ac-uueoxhj-shard-00-01.l1dei5j.mongodb.net:27017. The full response is { \"ok\" : 0, \"errmsg\" : \"you are over your space quota, using 514 MB of 512 MB\", \"code\" : 8000, \"codeName\" : \"AtlasError\" }\n",
      "\tat com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:179)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:293)\n",
      "\tat com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:255)\n",
      "\tat com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:99)\n",
      "\tat com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:444)\n",
      "\tat com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:72)\n",
      "\tat com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:200)\n",
      "\tat com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:269)\n",
      "\tat com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:131)\n",
      "\tat com.mongodb.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:419)\n",
      "\tat com.mongodb.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:257)\n",
      "\tat com.mongodb.operation.MixedBulkWriteOperation.access$700(MixedBulkWriteOperation.java:68)\n",
      "\tat com.mongodb.operation.MixedBulkWriteOperation$1.call(MixedBulkWriteOperation.java:201)\n",
      "\tat com.mongodb.operation.MixedBulkWriteOperation$1.call(MixedBulkWriteOperation.java:192)\n",
      "\tat com.mongodb.operation.OperationHelper.withReleasableConnection(OperationHelper.java:424)\n",
      "\tat com.mongodb.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:192)\n",
      "\tat com.mongodb.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:67)\n",
      "\tat com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:193)\n",
      "\tat com.mongodb.client.internal.MongoCollectionImpl.executeInsertMany(MongoCollectionImpl.java:520)\n",
      "\tat com.mongodb.client.internal.MongoCollectionImpl.insertMany(MongoCollectionImpl.java:504)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:121)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:119)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:119)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:118)\n",
      "\tat com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)\n",
      "\tat com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)\n",
      "\tat com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)\n",
      "\tat com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)\n",
      "\tat com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$1(MongoSpark.scala:118)\n",
      "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$1$adapted(MongoSpark.scala:117)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1011)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1011)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2278)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "23/04/14 01:07:54 ERROR TaskSetManager: Task 0 in stage 8.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o52.save.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 8.0 failed 1 times, most recent failure: Lost task 0.0 in stage 8.0 (TID 8) (josejoao-S540.Home executor driver): com.mongodb.MongoCommandException: Command failed with error 8000 (AtlasError): 'you are over your space quota, using 514 MB of 512 MB' on server ac-uueoxhj-shard-00-01.l1dei5j.mongodb.net:27017. The full response is { \"ok\" : 0, \"errmsg\" : \"you are over your space quota, using 514 MB of 512 MB\", \"code\" : 8000, \"codeName\" : \"AtlasError\" }\n\tat com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:179)\n\tat com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:293)\n\tat com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:255)\n\tat com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:99)\n\tat com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:444)\n\tat com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:72)\n\tat com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:200)\n\tat com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:269)\n\tat com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:131)\n\tat com.mongodb.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:419)\n\tat com.mongodb.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:257)\n\tat com.mongodb.operation.MixedBulkWriteOperation.access$700(MixedBulkWriteOperation.java:68)\n\tat com.mongodb.operation.MixedBulkWriteOperation$1.call(MixedBulkWriteOperation.java:201)\n\tat com.mongodb.operation.MixedBulkWriteOperation$1.call(MixedBulkWriteOperation.java:192)\n\tat com.mongodb.operation.OperationHelper.withReleasableConnection(OperationHelper.java:424)\n\tat com.mongodb.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:192)\n\tat com.mongodb.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:67)\n\tat com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:193)\n\tat com.mongodb.client.internal.MongoCollectionImpl.executeInsertMany(MongoCollectionImpl.java:520)\n\tat com.mongodb.client.internal.MongoCollectionImpl.insertMany(MongoCollectionImpl.java:504)\n\tat com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:121)\n\tat com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:119)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:119)\n\tat com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:118)\n\tat com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)\n\tat com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)\n\tat com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)\n\tat com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)\n\tat com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)\n\tat com.mongodb.spark.MongoSpark$.$anonfun$save$1(MongoSpark.scala:118)\n\tat com.mongodb.spark.MongoSpark$.$anonfun$save$1$adapted(MongoSpark.scala:117)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1011)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1011)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2278)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2238)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2259)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2278)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1011)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1009)\n\tat com.mongodb.spark.MongoSpark$.save(MongoSpark.scala:117)\n\tat com.mongodb.spark.MongoSpark$.save(MongoSpark.scala:159)\n\tat com.mongodb.spark.sql.DefaultSource.createRelation(DefaultSource.scala:73)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: com.mongodb.MongoCommandException: Command failed with error 8000 (AtlasError): 'you are over your space quota, using 514 MB of 512 MB' on server ac-uueoxhj-shard-00-01.l1dei5j.mongodb.net:27017. The full response is { \"ok\" : 0, \"errmsg\" : \"you are over your space quota, using 514 MB of 512 MB\", \"code\" : 8000, \"codeName\" : \"AtlasError\" }\n\tat com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:179)\n\tat com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:293)\n\tat com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:255)\n\tat com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:99)\n\tat com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:444)\n\tat com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:72)\n\tat com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:200)\n\tat com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:269)\n\tat com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:131)\n\tat com.mongodb.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:419)\n\tat com.mongodb.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:257)\n\tat com.mongodb.operation.MixedBulkWriteOperation.access$700(MixedBulkWriteOperation.java:68)\n\tat com.mongodb.operation.MixedBulkWriteOperation$1.call(MixedBulkWriteOperation.java:201)\n\tat com.mongodb.operation.MixedBulkWriteOperation$1.call(MixedBulkWriteOperation.java:192)\n\tat com.mongodb.operation.OperationHelper.withReleasableConnection(OperationHelper.java:424)\n\tat com.mongodb.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:192)\n\tat com.mongodb.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:67)\n\tat com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:193)\n\tat com.mongodb.client.internal.MongoCollectionImpl.executeInsertMany(MongoCollectionImpl.java:520)\n\tat com.mongodb.client.internal.MongoCollectionImpl.insertMany(MongoCollectionImpl.java:504)\n\tat com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:121)\n\tat com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:119)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:119)\n\tat com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:118)\n\tat com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)\n\tat com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)\n\tat com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)\n\tat com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)\n\tat com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)\n\tat com.mongodb.spark.MongoSpark$.$anonfun$save$1(MongoSpark.scala:118)\n\tat com.mongodb.spark.MongoSpark$.$anonfun$save$1$adapted(MongoSpark.scala:117)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1011)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1011)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2278)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_26796/830361897.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#     .getOrCreate()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mdf4\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"com.mongodb.spark.sql.DefaultSource\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"database\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"test\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"collection\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"inflation\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"uri\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"mongodb+srv://bigDataAdmin:admin@bigdatacluster.l1dei5j.mongodb.net/test\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"overwrite\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/DAA/lib/python3.7/site-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m    964\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 966\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    967\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    968\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DAA/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1322\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DAA/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DAA/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o52.save.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 8.0 failed 1 times, most recent failure: Lost task 0.0 in stage 8.0 (TID 8) (josejoao-S540.Home executor driver): com.mongodb.MongoCommandException: Command failed with error 8000 (AtlasError): 'you are over your space quota, using 514 MB of 512 MB' on server ac-uueoxhj-shard-00-01.l1dei5j.mongodb.net:27017. The full response is { \"ok\" : 0, \"errmsg\" : \"you are over your space quota, using 514 MB of 512 MB\", \"code\" : 8000, \"codeName\" : \"AtlasError\" }\n\tat com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:179)\n\tat com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:293)\n\tat com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:255)\n\tat com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:99)\n\tat com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:444)\n\tat com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:72)\n\tat com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:200)\n\tat com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:269)\n\tat com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:131)\n\tat com.mongodb.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:419)\n\tat com.mongodb.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:257)\n\tat com.mongodb.operation.MixedBulkWriteOperation.access$700(MixedBulkWriteOperation.java:68)\n\tat com.mongodb.operation.MixedBulkWriteOperation$1.call(MixedBulkWriteOperation.java:201)\n\tat com.mongodb.operation.MixedBulkWriteOperation$1.call(MixedBulkWriteOperation.java:192)\n\tat com.mongodb.operation.OperationHelper.withReleasableConnection(OperationHelper.java:424)\n\tat com.mongodb.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:192)\n\tat com.mongodb.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:67)\n\tat com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:193)\n\tat com.mongodb.client.internal.MongoCollectionImpl.executeInsertMany(MongoCollectionImpl.java:520)\n\tat com.mongodb.client.internal.MongoCollectionImpl.insertMany(MongoCollectionImpl.java:504)\n\tat com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:121)\n\tat com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:119)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:119)\n\tat com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:118)\n\tat com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)\n\tat com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)\n\tat com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)\n\tat com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)\n\tat com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)\n\tat com.mongodb.spark.MongoSpark$.$anonfun$save$1(MongoSpark.scala:118)\n\tat com.mongodb.spark.MongoSpark$.$anonfun$save$1$adapted(MongoSpark.scala:117)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1011)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1011)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2278)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2238)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2259)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2278)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1011)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1009)\n\tat com.mongodb.spark.MongoSpark$.save(MongoSpark.scala:117)\n\tat com.mongodb.spark.MongoSpark$.save(MongoSpark.scala:159)\n\tat com.mongodb.spark.sql.DefaultSource.createRelation(DefaultSource.scala:73)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: com.mongodb.MongoCommandException: Command failed with error 8000 (AtlasError): 'you are over your space quota, using 514 MB of 512 MB' on server ac-uueoxhj-shard-00-01.l1dei5j.mongodb.net:27017. The full response is { \"ok\" : 0, \"errmsg\" : \"you are over your space quota, using 514 MB of 512 MB\", \"code\" : 8000, \"codeName\" : \"AtlasError\" }\n\tat com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:179)\n\tat com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:293)\n\tat com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:255)\n\tat com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:99)\n\tat com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:444)\n\tat com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:72)\n\tat com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:200)\n\tat com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:269)\n\tat com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:131)\n\tat com.mongodb.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:419)\n\tat com.mongodb.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:257)\n\tat com.mongodb.operation.MixedBulkWriteOperation.access$700(MixedBulkWriteOperation.java:68)\n\tat com.mongodb.operation.MixedBulkWriteOperation$1.call(MixedBulkWriteOperation.java:201)\n\tat com.mongodb.operation.MixedBulkWriteOperation$1.call(MixedBulkWriteOperation.java:192)\n\tat com.mongodb.operation.OperationHelper.withReleasableConnection(OperationHelper.java:424)\n\tat com.mongodb.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:192)\n\tat com.mongodb.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:67)\n\tat com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:193)\n\tat com.mongodb.client.internal.MongoCollectionImpl.executeInsertMany(MongoCollectionImpl.java:520)\n\tat com.mongodb.client.internal.MongoCollectionImpl.insertMany(MongoCollectionImpl.java:504)\n\tat com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:121)\n\tat com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:119)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:119)\n\tat com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:118)\n\tat com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)\n\tat com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)\n\tat com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)\n\tat com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)\n\tat com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)\n\tat com.mongodb.spark.MongoSpark$.$anonfun$save$1(MongoSpark.scala:118)\n\tat com.mongodb.spark.MongoSpark$.$anonfun$save$1$adapted(MongoSpark.scala:117)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1011)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1011)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2278)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "\n",
    "# Criar uma sessão\n",
    "# spark = SparkSession.builder \\\n",
    "#     .appName(\"Ejemplo de conexión a MongoDB Atlas\") \\\n",
    "#     .config(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:2.4.0\") \\\n",
    "#     .getOrCreate()\n",
    "\n",
    "df4.write.format(\"com.mongodb.spark.sql.DefaultSource\").option(\"database\", \"test\").option(\"collection\", \"inflation\").option(\"uri\", \"mongodb+srv://bigDataAdmin:admin@bigdatacluster.l1dei5j.mongodb.net/test\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab43e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pymongo.mongo_client import MongoClient\n",
    "# from pymongo.server_api import ServerApi\n",
    "# import pandas as pd\n",
    "\n",
    "# uri = \"mongodb+srv://bigDataAdmin:admin@bigdatacluster.l1dei5j.mongodb.net/?retryWrites=true&w=majority\"\n",
    "\n",
    "# # Create a new client and connect to the server\n",
    "# client = MongoClient(uri, server_api=ServerApi('1'))\n",
    "\n",
    "# # Send a ping to confirm a successful connection\n",
    "# try:\n",
    "#     client.admin.command('ping')\n",
    "#     print(\"Pinged your deployment. You successfully connected to MongoDB!\")\n",
    "#     db = client['mergedParquet']\n",
    "#     collection = db['mergedParquet']\n",
    "\n",
    "#     #FIXME: o problema é que isto nao esta a dar\n",
    "#     # df5 = df4.toPandas()\n",
    "\n",
    "#     collection.insert_one(data_csv_ps.to_dict('split'))\n",
    "# except Exception as e:\n",
    "#     print(e)\n",
    "\n",
    "# client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6178a9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql import SparkSession\n",
    "# from pyspark.sql.functions import *\n",
    "# from pyspark.sql.types import *\n",
    "# import pymongo\n",
    "# import json\n",
    "\n",
    "# # Configurar a sessão do Spark\n",
    "# spark = SparkSession.builder \\\n",
    "#     .appName(\"MongoDBConnector\") \\\n",
    "#     .config(\"spark.mongodb.output.uri\", \"mongodb+srv://bigDataAdmin:admin@bigdatacluster/mergedParquet.mergedParquet2\") \\\n",
    "#     .config(\"spark.mongodb.output.database\", \"mergedParquet\") \\\n",
    "#     .config(\"spark.mongodb.output.collection\", \"mergedParquet2\") \\\n",
    "#     .getOrCreate()\n",
    "\n",
    "# # Ler arquivos Parquet e juntar em um dataframe\n",
    "# # Criar dataframes de teste\n",
    "# df1 = spark.createDataFrame([(2020, \"Q1\", \"A\", 100), (2020, \"Q2\", \"A\", 200), (2020, \"Q3\", \"A\", 300), (2020, \"Q4\", \"A\", 400)], [\"year\", \"quarter\", \"group1\", \"value1\"])\n",
    "# df2 = spark.createDataFrame([(2020, \"Q1\", \"B\", 150), (2020, \"Q2\", \"B\", 250), (2020, \"Q3\", \"B\", 350), (2020, \"Q4\", \"B\", 450)], [\"year\", \"quarter\", \"group2\", \"value2\"])\n",
    "# df3 = spark.createDataFrame([(2020, \"Q1\", \"C\", 75), (2020, \"Q2\", \"C\", 175), (2020, \"Q3\", \"C\", 275), (2020, \"Q4\", \"C\", 375)], [\"year\", \"quarter\", \"group3\", \"value3\"])\n",
    "\n",
    "# # Juntar dataframes em um dataframe único\n",
    "# df = df1.join(df2, on=['quarter', 'year'], how='inner').join(df3, on=['quarter', 'year'], how='inner')\n",
    "\n",
    "# # Renomear as colunas para evitar nomes duplicados\n",
    "# df = df.withColumnRenamed(\"group1\", \"group_name\")\n",
    "# df = df.withColumnRenamed(\"value1\", \"group_value\")\n",
    "\n",
    "\n",
    "# # Salvar dataframe em formato parquet\n",
    "# df = df.write.parquet(\"../parquetFiles2/teste.parquet\")\n",
    "\n",
    "# # Carregar dataframe a partir do arquivo parquet\n",
    "# df = spark.read.parquet(\"../parquetFiles2/teste.parquet\")\n",
    "\n",
    "# # Converter o dataframe para um RDD e, em seguida, para uma lista de dicionários\n",
    "# rdd = df.rdd.map(lambda x: dict(x))\n",
    "# data = [json.loads(json.dumps(d)) for d in rdd.collect()]\n",
    "\n",
    "# # Configurar a conexão com o MongoDB Atlas\n",
    "# mongo_uri = \"mongodb+srv://bigDataAdmin:admin@bigdatacluster/mergedParquet.mergedParquet2\"\n",
    "# database_name = \"mergedParquet\"\n",
    "# collection_name = \"mergedParquet2\"\n",
    "\n",
    "# client = pymongo.MongoClient(mongo_uri)\n",
    "# db = client[database_name]\n",
    "# collection = db[collection_name]\n",
    "\n",
    "# # Inserir os dados no MongoDB Atlas\n",
    "# collection.insert_many(data)\n",
    "\n",
    "# # Fechar a sessão do Spark\n",
    "# spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebee7df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
