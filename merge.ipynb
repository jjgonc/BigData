{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44bd1715-889a-4e4e-99fa-cc88f009bd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1b2228-ad9a-4e48-8925-2e986190e0ca",
   "metadata": {},
   "source": [
    "## Carregar os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "724437d3-0c7f-43c5-8707-a039d9a99887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar o dataset JSON\n",
    "with open('../datasets/HPI_master.json', 'r') as f:\n",
    "    json_data = json.load(f)\n",
    "\n",
    "# Converter o dataset JSON para um DataFrame do pandas\n",
    "data_json = pd.DataFrame(json_data)\n",
    "\n",
    "# Carregar o dataset CSV - Population size\n",
    "data_csv_ps = pd.read_csv(\"../datasets/cu.data.19.PopulationSize.csv\")\n",
    "data_csv_ps = pd.DataFrame(data_csv_ps)\n",
    "\n",
    "# Carregar o dataset CSV - \n",
    "data_csv_fb = pd.read_csv(\"../datasets/cu.data.11.USFoodBeverage.csv\")\n",
    "data_csv_fb = pd.DataFrame(data_csv_fb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21cdd734-e3ff-4b52-96dd-943191e78667",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Visualizar os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a881d113-6f6d-4d0b-8470-eacba8626d38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hpi_type</th>\n",
       "      <th>hpi_flavor</th>\n",
       "      <th>frequency</th>\n",
       "      <th>level</th>\n",
       "      <th>place_name</th>\n",
       "      <th>place_id</th>\n",
       "      <th>yr</th>\n",
       "      <th>period</th>\n",
       "      <th>index_nsa</th>\n",
       "      <th>index_sa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>traditional</td>\n",
       "      <td>purchase-only</td>\n",
       "      <td>monthly</td>\n",
       "      <td>USA or Census Division</td>\n",
       "      <td>East North Central Division</td>\n",
       "      <td>DV_ENC</td>\n",
       "      <td>1991</td>\n",
       "      <td>1</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>traditional</td>\n",
       "      <td>purchase-only</td>\n",
       "      <td>monthly</td>\n",
       "      <td>USA or Census Division</td>\n",
       "      <td>East North Central Division</td>\n",
       "      <td>DV_ENC</td>\n",
       "      <td>1991</td>\n",
       "      <td>2</td>\n",
       "      <td>100.91</td>\n",
       "      <td>100.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>traditional</td>\n",
       "      <td>purchase-only</td>\n",
       "      <td>monthly</td>\n",
       "      <td>USA or Census Division</td>\n",
       "      <td>East North Central Division</td>\n",
       "      <td>DV_ENC</td>\n",
       "      <td>1991</td>\n",
       "      <td>3</td>\n",
       "      <td>101.30</td>\n",
       "      <td>100.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>traditional</td>\n",
       "      <td>purchase-only</td>\n",
       "      <td>monthly</td>\n",
       "      <td>USA or Census Division</td>\n",
       "      <td>East North Central Division</td>\n",
       "      <td>DV_ENC</td>\n",
       "      <td>1991</td>\n",
       "      <td>4</td>\n",
       "      <td>101.69</td>\n",
       "      <td>100.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>traditional</td>\n",
       "      <td>purchase-only</td>\n",
       "      <td>monthly</td>\n",
       "      <td>USA or Census Division</td>\n",
       "      <td>East North Central Division</td>\n",
       "      <td>DV_ENC</td>\n",
       "      <td>1991</td>\n",
       "      <td>5</td>\n",
       "      <td>102.32</td>\n",
       "      <td>101.36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      hpi_type     hpi_flavor frequency                   level  \\\n",
       "0  traditional  purchase-only   monthly  USA or Census Division   \n",
       "1  traditional  purchase-only   monthly  USA or Census Division   \n",
       "2  traditional  purchase-only   monthly  USA or Census Division   \n",
       "3  traditional  purchase-only   monthly  USA or Census Division   \n",
       "4  traditional  purchase-only   monthly  USA or Census Division   \n",
       "\n",
       "                    place_name place_id    yr  period  index_nsa index_sa  \n",
       "0  East North Central Division   DV_ENC  1991       1     100.00    100.0  \n",
       "1  East North Central Division   DV_ENC  1991       2     100.91   100.96  \n",
       "2  East North Central Division   DV_ENC  1991       3     101.30   100.91  \n",
       "3  East North Central Division   DV_ENC  1991       4     101.69   100.98  \n",
       "4  East North Central Division   DV_ENC  1991       5     102.32   101.36  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_json.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25764e50-b56a-499c-89d5-de5489bb229a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>series_id</th>\n",
       "      <th>year</th>\n",
       "      <th>period</th>\n",
       "      <th>value</th>\n",
       "      <th>footnote_codes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CUURA000AA0</td>\n",
       "      <td>1986</td>\n",
       "      <td>M12</td>\n",
       "      <td>100.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CUURA000AA0</td>\n",
       "      <td>1987</td>\n",
       "      <td>M01</td>\n",
       "      <td>100.6</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CUURA000AA0</td>\n",
       "      <td>1987</td>\n",
       "      <td>M02</td>\n",
       "      <td>101.1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CUURA000AA0</td>\n",
       "      <td>1987</td>\n",
       "      <td>M03</td>\n",
       "      <td>101.6</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CUURA000AA0</td>\n",
       "      <td>1987</td>\n",
       "      <td>M04</td>\n",
       "      <td>102.2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     series_id  year period  value  footnote_codes\n",
       "0  CUURA000AA0  1986    M12  100.0             NaN\n",
       "1  CUURA000AA0  1987    M01  100.6             NaN\n",
       "2  CUURA000AA0  1987    M02  101.1             NaN\n",
       "3  CUURA000AA0  1987    M03  101.6             NaN\n",
       "4  CUURA000AA0  1987    M04  102.2             NaN"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_csv_ps.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "814080de-f4e2-4e2c-ac62-191d600577fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>series_id</th>\n",
       "      <th>year</th>\n",
       "      <th>period</th>\n",
       "      <th>value</th>\n",
       "      <th>footnote_codes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CUSR0000SAF</td>\n",
       "      <td>1967</td>\n",
       "      <td>M01</td>\n",
       "      <td>34.8</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CUSR0000SAF</td>\n",
       "      <td>1967</td>\n",
       "      <td>M02</td>\n",
       "      <td>34.7</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CUSR0000SAF</td>\n",
       "      <td>1967</td>\n",
       "      <td>M03</td>\n",
       "      <td>34.7</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CUSR0000SAF</td>\n",
       "      <td>1967</td>\n",
       "      <td>M04</td>\n",
       "      <td>34.6</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CUSR0000SAF</td>\n",
       "      <td>1967</td>\n",
       "      <td>M05</td>\n",
       "      <td>34.6</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     series_id  year period  value  footnote_codes\n",
       "0  CUSR0000SAF  1967    M01   34.8             NaN\n",
       "1  CUSR0000SAF  1967    M02   34.7             NaN\n",
       "2  CUSR0000SAF  1967    M03   34.7             NaN\n",
       "3  CUSR0000SAF  1967    M04   34.6             NaN\n",
       "4  CUSR0000SAF  1967    M05   34.6             NaN"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_csv_fb.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7be446a5",
   "metadata": {},
   "source": [
    "#### **Tipos de dados do dataset: Food & Beverage**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c43e660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 119198 entries, 0 to 119197\n",
      "Data columns (total 5 columns):\n",
      " #   Column          Non-Null Count   Dtype  \n",
      "---  ------          --------------   -----  \n",
      " 0   series_id       119198 non-null  object \n",
      " 1   year            119198 non-null  int64  \n",
      " 2   period          119198 non-null  object \n",
      " 3   value           119198 non-null  float64\n",
      " 4   footnote_codes  0 non-null       float64\n",
      "dtypes: float64(2), int64(1), object(2)\n",
      "memory usage: 4.5+ MB\n"
     ]
    }
   ],
   "source": [
    "data_csv_fb.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "684e7388-9e68-4a83-9250-4c7ec5b64b8c",
   "metadata": {},
   "source": [
    "#### **Tipos de dados do dataset: Population Size**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2a5708e-7465-4e2f-89c5-60fb1d52c64f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 84760 entries, 0 to 84759\n",
      "Data columns (total 5 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   series_id       84760 non-null  object \n",
      " 1   year            84760 non-null  int64  \n",
      " 2   period          84760 non-null  object \n",
      " 3   value           84760 non-null  float64\n",
      " 4   footnote_codes  0 non-null      float64\n",
      "dtypes: float64(2), int64(1), object(2)\n",
      "memory usage: 3.2+ MB\n"
     ]
    }
   ],
   "source": [
    "data_csv_ps.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45295a4-a374-4d00-9394-8f52546cd167",
   "metadata": {},
   "source": [
    "Conseguimos observar a existência de dados do tipo `object`. Poderá ser necessário tratar este tipo de dados."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "365eb01d-ba04-4fe3-9d87-f0c9352110e9",
   "metadata": {},
   "source": [
    "#### **Tipos de dados do dataset Json**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5dceddf7-6782-4613-a6d3-371c0ae0f69c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hpi_type       object\n",
       "hpi_flavor     object\n",
       "frequency      object\n",
       "level          object\n",
       "place_name     object\n",
       "place_id       object\n",
       "yr              int64\n",
       "period          int64\n",
       "index_nsa     float64\n",
       "index_sa       object\n",
       "dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_json.dtypes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1636d6e9",
   "metadata": {},
   "source": [
    "Remover linhas com ano anterior a 1977, uma vez que o dataset FoodBeverage começa nesse ano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75a9e4c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataframes size before]:\n",
      "\n",
      "\n",
      "FB:  series_id         119198\n",
      "year              119198\n",
      "period            119198\n",
      "value             119198\n",
      "footnote_codes         0\n",
      "dtype: int64 \n",
      "\n",
      "PS:  series_id         84760\n",
      "year              84760\n",
      "period            84760\n",
      "value             84760\n",
      "footnote_codes        0\n",
      "dtype: int64 \n",
      "\n",
      "HPI:  hpi_type      121462\n",
      "hpi_flavor    121462\n",
      "frequency     121462\n",
      "level         121462\n",
      "place_name    121462\n",
      "place_id      121462\n",
      "yr            121462\n",
      "period        121462\n",
      "index_nsa     121462\n",
      "index_sa      121462\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"[Dataframes size before]:\\n\\n\")\n",
    "print(\"FB: \", data_csv_fb.count(), \"\\n\")\n",
    "print(\"PS: \", data_csv_ps.count(), \"\\n\")\n",
    "print(\"HPI: \", data_json.count())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d117888e",
   "metadata": {},
   "source": [
    "Filtrar apenas as colunas com o ano igual ou superior a 1977, uma vez que nem todos os datasets contêm informação de anos anteriores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63383593",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_csv_fb = data_csv_fb[data_csv_fb['year'] > 1976]\n",
    "data_csv_ps = data_csv_ps[data_csv_ps['year'] > 1976]\n",
    "data_json = data_json[data_json['yr'] > 1976]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ccab45f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in data_json.iterrows():\n",
    "    if row['yr'] < 1977:\n",
    "        print(\"Falha detectada na linha\", index)\n",
    "\n",
    "for index, row in data_json.iterrows():\n",
    "    if row['yr'] < 1977:\n",
    "        print(\"Falha detectada na linha\", index)\n",
    "\n",
    "for index, row in data_json.iterrows():\n",
    "    if row['yr'] < 1977:\n",
    "        print(\"Falha detectada na linha\", index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e301144",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hpi_type</th>\n",
       "      <th>hpi_flavor</th>\n",
       "      <th>frequency</th>\n",
       "      <th>level</th>\n",
       "      <th>place_name</th>\n",
       "      <th>place_id</th>\n",
       "      <th>yr</th>\n",
       "      <th>period</th>\n",
       "      <th>index_nsa</th>\n",
       "      <th>index_sa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>traditional</td>\n",
       "      <td>purchase-only</td>\n",
       "      <td>monthly</td>\n",
       "      <td>USA or Census Division</td>\n",
       "      <td>East North Central Division</td>\n",
       "      <td>DV_ENC</td>\n",
       "      <td>1991</td>\n",
       "      <td>1</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>traditional</td>\n",
       "      <td>purchase-only</td>\n",
       "      <td>monthly</td>\n",
       "      <td>USA or Census Division</td>\n",
       "      <td>East North Central Division</td>\n",
       "      <td>DV_ENC</td>\n",
       "      <td>1991</td>\n",
       "      <td>2</td>\n",
       "      <td>100.91</td>\n",
       "      <td>100.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>traditional</td>\n",
       "      <td>purchase-only</td>\n",
       "      <td>monthly</td>\n",
       "      <td>USA or Census Division</td>\n",
       "      <td>East North Central Division</td>\n",
       "      <td>DV_ENC</td>\n",
       "      <td>1991</td>\n",
       "      <td>3</td>\n",
       "      <td>101.30</td>\n",
       "      <td>100.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>traditional</td>\n",
       "      <td>purchase-only</td>\n",
       "      <td>monthly</td>\n",
       "      <td>USA or Census Division</td>\n",
       "      <td>East North Central Division</td>\n",
       "      <td>DV_ENC</td>\n",
       "      <td>1991</td>\n",
       "      <td>4</td>\n",
       "      <td>101.69</td>\n",
       "      <td>100.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>traditional</td>\n",
       "      <td>purchase-only</td>\n",
       "      <td>monthly</td>\n",
       "      <td>USA or Census Division</td>\n",
       "      <td>East North Central Division</td>\n",
       "      <td>DV_ENC</td>\n",
       "      <td>1991</td>\n",
       "      <td>5</td>\n",
       "      <td>102.32</td>\n",
       "      <td>101.36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      hpi_type     hpi_flavor frequency                   level  \\\n",
       "0  traditional  purchase-only   monthly  USA or Census Division   \n",
       "1  traditional  purchase-only   monthly  USA or Census Division   \n",
       "2  traditional  purchase-only   monthly  USA or Census Division   \n",
       "3  traditional  purchase-only   monthly  USA or Census Division   \n",
       "4  traditional  purchase-only   monthly  USA or Census Division   \n",
       "\n",
       "                    place_name place_id    yr  period  index_nsa index_sa  \n",
       "0  East North Central Division   DV_ENC  1991       1     100.00    100.0  \n",
       "1  East North Central Division   DV_ENC  1991       2     100.91   100.96  \n",
       "2  East North Central Division   DV_ENC  1991       3     101.30   100.91  \n",
       "3  East North Central Division   DV_ENC  1991       4     101.69   100.98  \n",
       "4  East North Central Division   DV_ENC  1991       5     102.32   101.36  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_json.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d1a10df4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataframe size after]: FB:  series_id         91350\n",
      "year              91350\n",
      "period            91350\n",
      "value             91350\n",
      "footnote_codes        0\n",
      "dtype: int64 PS:  series_id         84760\n",
      "year              84760\n",
      "period            84760\n",
      "value             84760\n",
      "footnote_codes        0\n",
      "dtype: int64 HPI:  hpi_type      120794\n",
      "hpi_flavor    120794\n",
      "frequency     120794\n",
      "level         120794\n",
      "place_name    120794\n",
      "place_id      120794\n",
      "yr            120794\n",
      "period        120794\n",
      "index_nsa     120794\n",
      "index_sa      120794\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"[Dataframe size after]: FB: \", data_csv_fb.count(), \"PS: \", data_csv_ps.count(), \"HPI: \", data_json.count())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b71d0813",
   "metadata": {},
   "source": [
    "#### **Verificar a existencia de Missing Values em cada um dos Dataframes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "69e36e6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values in House Price dataset:\n",
      "hpi_type      0\n",
      "hpi_flavor    0\n",
      "frequency     0\n",
      "level         0\n",
      "place_name    0\n",
      "place_id      0\n",
      "yr            0\n",
      "period        0\n",
      "index_nsa     0\n",
      "index_sa      0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of missing values in House Price dataset:\")\n",
    "json_MV = data_json.isnull().sum()\n",
    "print(json_MV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7555c8f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values in FoodBeverage dataset:\n",
      "series_id             0\n",
      "year                  0\n",
      "period                0\n",
      "value                 0\n",
      "footnote_codes    91350\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of missing values in FoodBeverage dataset:\")\n",
    "foodBeverage_MV = data_csv_fb.isnull().sum()\n",
    "print(foodBeverage_MV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d9c197d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values in Population Size dataset:\n",
      "series_id             0\n",
      "year                  0\n",
      "period                0\n",
      "value                 0\n",
      "footnote_codes    84760\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of missing values in Population Size dataset:\")\n",
    "populationSize_MV = data_csv_ps.isnull().sum()\n",
    "print(populationSize_MV)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c16a2b15-7462-4349-a2d3-0d68369901da",
   "metadata": {},
   "source": [
    "#### **Verificar os valores da coluna `period`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c8599a5c-94a3-4e54-8770-0398956ec23b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 'period' column contains the following values:\n",
      "['M12' 'M01' 'M02' 'M03' 'M04' 'M05' 'M06' 'M07' 'M08' 'M09' 'M10' 'M11'\n",
      " 'M13' 'S01' 'S02' 'S03']\n"
     ]
    }
   ],
   "source": [
    "period_values = data_csv_ps['period'].unique()\n",
    "print(\"The 'period' column contains the following values:\")\n",
    "print(period_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627fa9cc-b961-45d6-9742-2039fb3ee8b2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Tratamento dos Dados"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "964749c0-56f4-403d-8863-9f998d8fec03",
   "metadata": {},
   "source": [
    "#### **Transformar `index_sa` em dados numéricos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8f89bc6b-623d-44c6-84e2-1c1703e073f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_json['index_sa'] = data_json['index_sa'].astype(float)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7f5c3656-876d-41a4-a29e-00fd05c71365",
   "metadata": {},
   "source": [
    "#### **Transformar `period` em dados numéricos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "199ef4f2-f8e0-447c-99e4-57a0e715aa50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         1\n",
       "1         2\n",
       "2         3\n",
       "3         4\n",
       "4         5\n",
       "         ..\n",
       "121457    4\n",
       "121458    1\n",
       "121459    2\n",
       "121460    3\n",
       "121461    4\n",
       "Name: period, Length: 120794, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dicionário para mapear o período número ao valor da string\n",
    "period_map = {\n",
    "    'M01': 1,\n",
    "    'M02': 2,\n",
    "    'M03': 3,\n",
    "    'M04': 4,\n",
    "    'M05': 5,\n",
    "    'M06': 6,\n",
    "    'M07': 7,\n",
    "    'M08': 8,\n",
    "    'M09': 9,\n",
    "    'M10': 10,\n",
    "    'M11': 11,\n",
    "    'M12': 12,\n",
    "    'M13': 13,\n",
    "    'S01': 14,\n",
    "    'S02': 15,\n",
    "    'S03': 16\n",
    "}\n",
    "\n",
    "data_csv_ps['period'] = data_csv_ps[\"period\"].replace(period_map)\n",
    "data_csv_fb['period'] = data_csv_fb[\"period\"].replace(period_map)\n",
    "data_json['period'].astype(str).astype(int)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "97c7459a-e86b-43a0-bc77-be163af43316",
   "metadata": {},
   "source": [
    "#### **Remover a coluna `footnote_codes`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "957b5975-452b-4d27-b68f-198d9aee9071",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_csv_ps.drop(columns=[\"footnote_codes\"], inplace=True)\n",
    "data_csv_fb.drop(columns=[\"footnote_codes\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "41f042f6-bab8-427d-b856-83554b2efbdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>series_id</th>\n",
       "      <th>year</th>\n",
       "      <th>period</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CUURA000AA0</td>\n",
       "      <td>1986</td>\n",
       "      <td>12</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CUURA000AA0</td>\n",
       "      <td>1987</td>\n",
       "      <td>1</td>\n",
       "      <td>100.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CUURA000AA0</td>\n",
       "      <td>1987</td>\n",
       "      <td>2</td>\n",
       "      <td>101.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CUURA000AA0</td>\n",
       "      <td>1987</td>\n",
       "      <td>3</td>\n",
       "      <td>101.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CUURA000AA0</td>\n",
       "      <td>1987</td>\n",
       "      <td>4</td>\n",
       "      <td>102.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     series_id  year  period  value\n",
       "0  CUURA000AA0  1986      12  100.0\n",
       "1  CUURA000AA0  1987       1  100.6\n",
       "2  CUURA000AA0  1987       2  101.1\n",
       "3  CUURA000AA0  1987       3  101.6\n",
       "4  CUURA000AA0  1987       4  102.2"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_csv_ps.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1e49c821-24e5-4c97-af55-fbe33f4430d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>series_id</th>\n",
       "      <th>year</th>\n",
       "      <th>period</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>CUSR0000SAF</td>\n",
       "      <td>1977</td>\n",
       "      <td>1</td>\n",
       "      <td>63.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>CUSR0000SAF</td>\n",
       "      <td>1977</td>\n",
       "      <td>2</td>\n",
       "      <td>64.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>CUSR0000SAF</td>\n",
       "      <td>1977</td>\n",
       "      <td>3</td>\n",
       "      <td>64.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>CUSR0000SAF</td>\n",
       "      <td>1977</td>\n",
       "      <td>4</td>\n",
       "      <td>65.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>CUSR0000SAF</td>\n",
       "      <td>1977</td>\n",
       "      <td>5</td>\n",
       "      <td>65.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       series_id  year  period  value\n",
       "120  CUSR0000SAF  1977       1   63.1\n",
       "121  CUSR0000SAF  1977       2   64.2\n",
       "122  CUSR0000SAF  1977       3   64.5\n",
       "123  CUSR0000SAF  1977       4   65.2\n",
       "124  CUSR0000SAF  1977       5   65.5"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_csv_fb.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d3b6208f-8ea5-4adf-a8af-8059c6d4f820",
   "metadata": {},
   "source": [
    "#### **Tratar Nan Values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cb6ed618-ed38-4d1e-b568-d9c547ee3799",
   "metadata": {},
   "outputs": [],
   "source": [
    "median_value = data_json['index_sa'].median()\n",
    "data_json['index_sa'] = data_json['index_sa'].fillna(median_value)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0045c4a4",
   "metadata": {},
   "source": [
    "#### **Renomear colunas**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "47595164",
   "metadata": {},
   "source": [
    "Estas colunas são necessárias renomear porque irão dar erro ao efetuar o merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "16a6a211",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_json.rename(columns = {'yr':'year'}, inplace = True)\n",
    "data_csv_fb.rename(columns = {'value':'valueFoodBeverage'}, inplace = True)\n",
    "data_csv_ps.rename(columns = {'value':'valuePopSize'}, inplace = True)\n",
    "data_csv_fb.rename(columns = {'series_id':'idFoodBeverage'}, inplace = True)\n",
    "data_csv_fb.rename(columns = {'series_id':'idPopSize'}, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1843ef-ffd5-43be-9dba-e5b245f46a27",
   "metadata": {},
   "source": [
    "## Converter para Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aa22fa1f-7eef-4e81-a810-6fb64a310c9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hpi_type</th>\n",
       "      <th>hpi_flavor</th>\n",
       "      <th>frequency</th>\n",
       "      <th>level</th>\n",
       "      <th>place_name</th>\n",
       "      <th>place_id</th>\n",
       "      <th>year</th>\n",
       "      <th>period</th>\n",
       "      <th>index_nsa</th>\n",
       "      <th>index_sa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>traditional</td>\n",
       "      <td>purchase-only</td>\n",
       "      <td>monthly</td>\n",
       "      <td>USA or Census Division</td>\n",
       "      <td>East North Central Division</td>\n",
       "      <td>DV_ENC</td>\n",
       "      <td>1991</td>\n",
       "      <td>1</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>traditional</td>\n",
       "      <td>purchase-only</td>\n",
       "      <td>monthly</td>\n",
       "      <td>USA or Census Division</td>\n",
       "      <td>East North Central Division</td>\n",
       "      <td>DV_ENC</td>\n",
       "      <td>1991</td>\n",
       "      <td>2</td>\n",
       "      <td>100.91</td>\n",
       "      <td>100.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>traditional</td>\n",
       "      <td>purchase-only</td>\n",
       "      <td>monthly</td>\n",
       "      <td>USA or Census Division</td>\n",
       "      <td>East North Central Division</td>\n",
       "      <td>DV_ENC</td>\n",
       "      <td>1991</td>\n",
       "      <td>3</td>\n",
       "      <td>101.30</td>\n",
       "      <td>100.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>traditional</td>\n",
       "      <td>purchase-only</td>\n",
       "      <td>monthly</td>\n",
       "      <td>USA or Census Division</td>\n",
       "      <td>East North Central Division</td>\n",
       "      <td>DV_ENC</td>\n",
       "      <td>1991</td>\n",
       "      <td>4</td>\n",
       "      <td>101.69</td>\n",
       "      <td>100.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>traditional</td>\n",
       "      <td>purchase-only</td>\n",
       "      <td>monthly</td>\n",
       "      <td>USA or Census Division</td>\n",
       "      <td>East North Central Division</td>\n",
       "      <td>DV_ENC</td>\n",
       "      <td>1991</td>\n",
       "      <td>5</td>\n",
       "      <td>102.32</td>\n",
       "      <td>101.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121457</th>\n",
       "      <td>developmental</td>\n",
       "      <td>purchase-only</td>\n",
       "      <td>quarterly</td>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>PR</td>\n",
       "      <td>2021</td>\n",
       "      <td>4</td>\n",
       "      <td>185.03</td>\n",
       "      <td>183.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121458</th>\n",
       "      <td>developmental</td>\n",
       "      <td>purchase-only</td>\n",
       "      <td>quarterly</td>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>PR</td>\n",
       "      <td>2022</td>\n",
       "      <td>1</td>\n",
       "      <td>185.82</td>\n",
       "      <td>190.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121459</th>\n",
       "      <td>developmental</td>\n",
       "      <td>purchase-only</td>\n",
       "      <td>quarterly</td>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>PR</td>\n",
       "      <td>2022</td>\n",
       "      <td>2</td>\n",
       "      <td>179.30</td>\n",
       "      <td>179.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121460</th>\n",
       "      <td>developmental</td>\n",
       "      <td>purchase-only</td>\n",
       "      <td>quarterly</td>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>PR</td>\n",
       "      <td>2022</td>\n",
       "      <td>3</td>\n",
       "      <td>190.09</td>\n",
       "      <td>187.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121461</th>\n",
       "      <td>developmental</td>\n",
       "      <td>purchase-only</td>\n",
       "      <td>quarterly</td>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>PR</td>\n",
       "      <td>2022</td>\n",
       "      <td>4</td>\n",
       "      <td>199.98</td>\n",
       "      <td>198.91</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120794 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             hpi_type     hpi_flavor  frequency                   level  \\\n",
       "0         traditional  purchase-only    monthly  USA or Census Division   \n",
       "1         traditional  purchase-only    monthly  USA or Census Division   \n",
       "2         traditional  purchase-only    monthly  USA or Census Division   \n",
       "3         traditional  purchase-only    monthly  USA or Census Division   \n",
       "4         traditional  purchase-only    monthly  USA or Census Division   \n",
       "...               ...            ...        ...                     ...   \n",
       "121457  developmental  purchase-only  quarterly             Puerto Rico   \n",
       "121458  developmental  purchase-only  quarterly             Puerto Rico   \n",
       "121459  developmental  purchase-only  quarterly             Puerto Rico   \n",
       "121460  developmental  purchase-only  quarterly             Puerto Rico   \n",
       "121461  developmental  purchase-only  quarterly             Puerto Rico   \n",
       "\n",
       "                         place_name place_id  year  period  index_nsa  \\\n",
       "0       East North Central Division   DV_ENC  1991       1     100.00   \n",
       "1       East North Central Division   DV_ENC  1991       2     100.91   \n",
       "2       East North Central Division   DV_ENC  1991       3     101.30   \n",
       "3       East North Central Division   DV_ENC  1991       4     101.69   \n",
       "4       East North Central Division   DV_ENC  1991       5     102.32   \n",
       "...                             ...      ...   ...     ...        ...   \n",
       "121457                  Puerto Rico       PR  2021       4     185.03   \n",
       "121458                  Puerto Rico       PR  2022       1     185.82   \n",
       "121459                  Puerto Rico       PR  2022       2     179.30   \n",
       "121460                  Puerto Rico       PR  2022       3     190.09   \n",
       "121461                  Puerto Rico       PR  2022       4     199.98   \n",
       "\n",
       "        index_sa  \n",
       "0         100.00  \n",
       "1         100.96  \n",
       "2         100.91  \n",
       "3         100.98  \n",
       "4         101.36  \n",
       "...          ...  \n",
       "121457    183.17  \n",
       "121458    190.35  \n",
       "121459    179.96  \n",
       "121460    187.85  \n",
       "121461    198.91  \n",
       "\n",
       "[120794 rows x 10 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pq1 = data_csv_fb.to_parquet('../parquetFiles/data_fb.parquet')\n",
    "pq2 = data_csv_ps.to_parquet('../parquetFiles/data_ps.parquet')\n",
    "pq3 = data_json.to_parquet('../parquetFiles/data_json.parquet')\n",
    "\n",
    "pd.read_parquet('../parquetFiles/data_fb.parquet')\n",
    "pd.read_parquet('../parquetFiles/data_ps.parquet')\n",
    "pd.read_parquet('../parquetFiles/data_json.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814f2464-dda7-4921-aaf3-98366a612c9c",
   "metadata": {},
   "source": [
    "## Realizar o Merge dos datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0a3affdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/03 16:36:38 WARN Utils: Your hostname, josejoao-S540 resolves to a loopback address: 127.0.1.1; using 192.168.1.183 instead (on interface wlp0s20f3)\n",
      "23/05/03 16:36:38 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      ":: loading settings :: url = jar:file:/home/josejoao/anaconda3/envs/DAA/lib/python3.7/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/josejoao/.ivy2/cache\n",
      "The jars for the packages stored in: /home/josejoao/.ivy2/jars\n",
      "org.mongodb.spark#mongo-spark-connector_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-f44775f9-942f-422d-a83e-9bf387068d35;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.mongodb.spark#mongo-spark-connector_2.12;2.4.0 in central\n",
      "\tfound org.mongodb#mongo-java-driver;3.9.0 in central\n",
      ":: resolution report :: resolve 154ms :: artifacts dl 6ms\n",
      "\t:: modules in use:\n",
      "\torg.mongodb#mongo-java-driver;3.9.0 from central in [default]\n",
      "\torg.mongodb.spark#mongo-spark-connector_2.12;2.4.0 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   2   |   0   |   0   |   0   ||   2   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-f44775f9-942f-422d-a83e-9bf387068d35\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 2 already retrieved (0kB/6ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/03 16:36:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from pyspark.sql.functions import *\n",
    "# from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as Func\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import expr\n",
    "from pyspark.sql.types import *\n",
    "# spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Conexao ao MongoDB Atlas\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:2.4.0\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "pq1 = spark.read.format(\"parquet\").load('../parquetFiles/data_fb.parquet')\n",
    "pq2 = spark.read.format(\"parquet\").load('../parquetFiles/data_ps.parquet')\n",
    "pq3 = spark.read.format(\"parquet\").load('../parquetFiles/data_json.parquet')\n",
    "\n",
    "df4 = pq1.join(pq2, on=['period', 'year'], how='inner').join(pq3, on=['period', 'year'], how='inner')\n",
    "\n",
    "type(df4)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a018fadc",
   "metadata": {},
   "source": [
    "#### **Escrever o dataset com o merge efetuado para o formato Parquet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e465acd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pq_merged = df4.write.parquet('../parquetFiles/merged.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "380020de",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = df4.drop(\"__index_level_0__\", \"__index_level_0__\", \"__index_level_0__\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5731d5a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- period: long (nullable = true)\n",
      " |-- year: long (nullable = true)\n",
      " |-- idFoodBeverage: string (nullable = true)\n",
      " |-- valueFoodBeverage: double (nullable = true)\n",
      " |-- series_id: string (nullable = true)\n",
      " |-- valuePopSize: double (nullable = true)\n",
      " |-- hpi_type: string (nullable = true)\n",
      " |-- hpi_flavor: string (nullable = true)\n",
      " |-- frequency: string (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- place_name: string (nullable = true)\n",
      " |-- place_id: string (nullable = true)\n",
      " |-- index_nsa: double (nullable = true)\n",
      " |-- index_sa: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df4.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e71575ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(period=1, year=1978, idFoodBeverage='CUSR0000SAF', valueFoodBeverage=68.1, series_id='CUURD000SETB01', valuePopSize=51.2, hpi_type='traditional', hpi_flavor='all-transactions', frequency='quarterly', level='USA or Census Division', place_name='United States', place_id='USA', index_nsa=79.58, index_sa=180.51),\n",
       " Row(period=1, year=1978, idFoodBeverage='CUSR0000SAF', valueFoodBeverage=68.1, series_id='CUURD000SETB01', valuePopSize=51.2, hpi_type='traditional', hpi_flavor='all-transactions', frequency='quarterly', level='USA or Census Division', place_name='West South Central Division', place_id='DV_WSC', index_nsa=75.65, index_sa=180.51),\n",
       " Row(period=1, year=1978, idFoodBeverage='CUSR0000SAF', valueFoodBeverage=68.1, series_id='CUURD000SETB01', valuePopSize=51.2, hpi_type='traditional', hpi_flavor='all-transactions', frequency='quarterly', level='USA or Census Division', place_name='West North Central Division', place_id='DV_WNC', index_nsa=80.99, index_sa=180.51),\n",
       " Row(period=1, year=1978, idFoodBeverage='CUSR0000SAF', valueFoodBeverage=68.1, series_id='CUURD000SETB01', valuePopSize=51.2, hpi_type='traditional', hpi_flavor='all-transactions', frequency='quarterly', level='USA or Census Division', place_name='South Atlantic Division', place_id='DV_SA', index_nsa=81.3, index_sa=180.51),\n",
       " Row(period=1, year=1978, idFoodBeverage='CUSR0000SAF', valueFoodBeverage=68.1, series_id='CUURD000SETB01', valuePopSize=51.2, hpi_type='traditional', hpi_flavor='all-transactions', frequency='quarterly', level='USA or Census Division', place_name='Pacific Division', place_id='DV_PAC', index_nsa=73.88, index_sa=180.51),\n",
       " Row(period=1, year=1978, idFoodBeverage='CUSR0000SAF', valueFoodBeverage=68.1, series_id='CUURD000SETB01', valuePopSize=51.2, hpi_type='traditional', hpi_flavor='all-transactions', frequency='quarterly', level='USA or Census Division', place_name='New England Division', place_id='DV_NE', index_nsa=77.6, index_sa=180.51),\n",
       " Row(period=1, year=1978, idFoodBeverage='CUSR0000SAF', valueFoodBeverage=68.1, series_id='CUURD000SETB01', valuePopSize=51.2, hpi_type='traditional', hpi_flavor='all-transactions', frequency='quarterly', level='USA or Census Division', place_name='Mountain Division', place_id='DV_MT', index_nsa=77.61, index_sa=180.51),\n",
       " Row(period=1, year=1978, idFoodBeverage='CUSR0000SAF', valueFoodBeverage=68.1, series_id='CUURD000SETB01', valuePopSize=51.2, hpi_type='traditional', hpi_flavor='all-transactions', frequency='quarterly', level='USA or Census Division', place_name='Middle Atlantic Division', place_id='DV_MA', index_nsa=80.56, index_sa=180.51),\n",
       " Row(period=1, year=1978, idFoodBeverage='CUSR0000SAF', valueFoodBeverage=68.1, series_id='CUURD000SETB01', valuePopSize=51.2, hpi_type='traditional', hpi_flavor='all-transactions', frequency='quarterly', level='USA or Census Division', place_name='East South Central Division', place_id='DV_ESC', index_nsa=84.75, index_sa=180.51),\n",
       " Row(period=1, year=1978, idFoodBeverage='CUSR0000SAF', valueFoodBeverage=68.1, series_id='CUURD000SETB01', valuePopSize=51.2, hpi_type='traditional', hpi_flavor='all-transactions', frequency='quarterly', level='USA or Census Division', place_name='East North Central Division', place_id='DV_ENC', index_nsa=83.0, index_sa=180.51)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4.head(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "639048b2",
   "metadata": {},
   "source": [
    "#### Tratamento dos dados para redução do tamanho do dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1e25e758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remover os valores nulos e repetidos\n",
    "df4 = df4.dropna()\n",
    "df4 = df4.dropDuplicates()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5976782d",
   "metadata": {},
   "source": [
    "----\n",
    "## **Conversão para formato Documental**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d75b0f9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/27 11:41:40 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 26:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/27 11:42:00 ERROR Utils: uncaught error in thread Spark Context Cleaner, stopping SparkContext\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.ContextCleaner$$Lambda$900/0x000000084075e040.get$Lambda(Unknown Source)\n",
      "\tat java.base/java.lang.invoke.DirectMethodHandle$Holder.invokeStatic(DirectMethodHandle$Holder)\n",
      "\tat java.base/java.lang.invoke.Invokers$Holder.linkToTargetMethod(Invokers$Holder)\n",
      "\tat org.apache.spark.ContextCleaner.$anonfun$keepCleaning$1(ContextCleaner.scala:195)\n",
      "\tat org.apache.spark.ContextCleaner$$Lambda$818/0x00000008406dd840.apply$mcV$sp(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1446)\n",
      "\tat org.apache.spark.ContextCleaner.org$apache$spark$ContextCleaner$$keepCleaning(ContextCleaner.scala:189)\n",
      "\tat org.apache.spark.ContextCleaner$$anon$1.run(ContextCleaner.scala:79)\n",
      "23/04/27 11:42:00 ERROR Executor: Exception in task 0.0 in stage 26.0 (TID 26)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.unsafe.types.UTF8String.fromAddress(UTF8String.java:132)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.getUTF8String(UnsafeRow.java:406)\n",
      "\tat org.apache.spark.sql.catalyst.InternalRow$.$anonfun$getAccessor$8(InternalRow.scala:141)\n",
      "\tat org.apache.spark.sql.catalyst.InternalRow$.$anonfun$getAccessor$8$adapted(InternalRow.scala:141)\n",
      "\tat org.apache.spark.sql.catalyst.InternalRow$$$Lambda$2929/0x00000008412c8840.apply(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.InternalRow$.$anonfun$getAccessor$16(InternalRow.scala:157)\n",
      "\tat org.apache.spark.sql.catalyst.InternalRow$.$anonfun$getAccessor$16$adapted(InternalRow.scala:153)\n",
      "\tat org.apache.spark.sql.catalyst.InternalRow$$$Lambda$1995/0x0000000840dfe040.apply(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.BoundReference.eval(BoundAttribute.scala:40)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.CreateNamedStruct.$anonfun$eval$2(complexTypeCreator.scala:470)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.CreateNamedStruct$$Lambda$3566/0x0000000841559840.apply(Unknown Source)\n",
      "\tat scala.collection.immutable.List.map(List.scala:297)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.CreateNamedStruct.eval(complexTypeCreator.scala:470)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.aggregate.Collect.update(collect.scala:51)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.aggregate.Collect.update(collect.scala:37)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.aggregate.TypedImperativeAggregate.update(interfaces.scala:583)\n",
      "\tat org.apache.spark.sql.execution.aggregate.AggregationIterator$$anonfun$1.$anonfun$applyOrElse$2(AggregationIterator.scala:197)\n",
      "\tat org.apache.spark.sql.execution.aggregate.AggregationIterator$$anonfun$1.$anonfun$applyOrElse$2$adapted(AggregationIterator.scala:197)\n",
      "\tat org.apache.spark.sql.execution.aggregate.AggregationIterator$$anonfun$1$$Lambda$3551/0x0000000841545040.apply(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.aggregate.AggregationIterator.$anonfun$generateProcessRow$7(AggregationIterator.scala:214)\n",
      "\tat org.apache.spark.sql.execution.aggregate.AggregationIterator.$anonfun$generateProcessRow$7$adapted(AggregationIterator.scala:208)\n",
      "\tat org.apache.spark.sql.execution.aggregate.AggregationIterator$$Lambda$3553/0x0000000841546840.apply(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.processInputs(ObjectAggregationIterator.scala:169)\n",
      "\tat org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.<init>(ObjectAggregationIterator.scala:83)\n",
      "\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1(ObjectHashAggregateExec.scala:114)\n",
      "\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1$adapted(ObjectHashAggregateExec.scala:90)\n",
      "\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec$$Lambda$3525/0x0000000841532840.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:877)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:877)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$3526/0x0000000841535040.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "23/04/27 11:42:00 ERROR Utils: throw uncaught fatal error in thread Spark Context Cleaner\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.ContextCleaner$$Lambda$900/0x000000084075e040.get$Lambda(Unknown Source)\n",
      "\tat java.base/java.lang.invoke.DirectMethodHandle$Holder.invokeStatic(DirectMethodHandle$Holder)\n",
      "\tat java.base/java.lang.invoke.Invokers$Holder.linkToTargetMethod(Invokers$Holder)\n",
      "\tat org.apache.spark.ContextCleaner.$anonfun$keepCleaning$1(ContextCleaner.scala:195)\n",
      "\tat org.apache.spark.ContextCleaner$$Lambda$818/0x00000008406dd840.apply$mcV$sp(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1446)\n",
      "\tat org.apache.spark.ContextCleaner.org$apache$spark$ContextCleaner$$keepCleaning(ContextCleaner.scala:189)\n",
      "\tat org.apache.spark.ContextCleaner$$anon$1.run(ContextCleaner.scala:79)\n",
      "23/04/27 11:42:00 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 0.0 in stage 26.0 (TID 26),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.unsafe.types.UTF8String.fromAddress(UTF8String.java:132)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.getUTF8String(UnsafeRow.java:406)\n",
      "\tat org.apache.spark.sql.catalyst.InternalRow$.$anonfun$getAccessor$8(InternalRow.scala:141)\n",
      "\tat org.apache.spark.sql.catalyst.InternalRow$.$anonfun$getAccessor$8$adapted(InternalRow.scala:141)\n",
      "\tat org.apache.spark.sql.catalyst.InternalRow$$$Lambda$2929/0x00000008412c8840.apply(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.InternalRow$.$anonfun$getAccessor$16(InternalRow.scala:157)\n",
      "\tat org.apache.spark.sql.catalyst.InternalRow$.$anonfun$getAccessor$16$adapted(InternalRow.scala:153)\n",
      "\tat org.apache.spark.sql.catalyst.InternalRow$$$Lambda$1995/0x0000000840dfe040.apply(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.BoundReference.eval(BoundAttribute.scala:40)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.CreateNamedStruct.$anonfun$eval$2(complexTypeCreator.scala:470)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.CreateNamedStruct$$Lambda$3566/0x0000000841559840.apply(Unknown Source)\n",
      "\tat scala.collection.immutable.List.map(List.scala:297)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.CreateNamedStruct.eval(complexTypeCreator.scala:470)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.aggregate.Collect.update(collect.scala:51)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.aggregate.Collect.update(collect.scala:37)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.aggregate.TypedImperativeAggregate.update(interfaces.scala:583)\n",
      "\tat org.apache.spark.sql.execution.aggregate.AggregationIterator$$anonfun$1.$anonfun$applyOrElse$2(AggregationIterator.scala:197)\n",
      "\tat org.apache.spark.sql.execution.aggregate.AggregationIterator$$anonfun$1.$anonfun$applyOrElse$2$adapted(AggregationIterator.scala:197)\n",
      "\tat org.apache.spark.sql.execution.aggregate.AggregationIterator$$anonfun$1$$Lambda$3551/0x0000000841545040.apply(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.aggregate.AggregationIterator.$anonfun$generateProcessRow$7(AggregationIterator.scala:214)\n",
      "\tat org.apache.spark.sql.execution.aggregate.AggregationIterator.$anonfun$generateProcessRow$7$adapted(AggregationIterator.scala:208)\n",
      "\tat org.apache.spark.sql.execution.aggregate.AggregationIterator$$Lambda$3553/0x0000000841546840.apply(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.processInputs(ObjectAggregationIterator.scala:169)\n",
      "\tat org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.<init>(ObjectAggregationIterator.scala:83)\n",
      "\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1(ObjectHashAggregateExec.scala:114)\n",
      "\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1$adapted(ObjectHashAggregateExec.scala:90)\n",
      "\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec$$Lambda$3525/0x0000000841532840.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:877)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:877)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$3526/0x0000000841535040.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread \"Spark Context Cleaner\" java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.ContextCleaner$$Lambda$900/0x000000084075e040.get$Lambda(Unknown Source)\n",
      "\tat java.base/java.lang.invoke.DirectMethodHandle$Holder.invokeStatic(DirectMethodHandle$Holder)\n",
      "\tat java.base/java.lang.invoke.Invokers$Holder.linkToTargetMethod(Invokers$Holder)\n",
      "\tat org.apache.spark.ContextCleaner.$anonfun$keepCleaning$1(ContextCleaner.scala:195)\n",
      "\tat org.apache.spark.ContextCleaner$$Lambda$818/0x00000008406dd840.apply$mcV$sp(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1446)\n",
      "\tat org.apache.spark.ContextCleaner.org$apache$spark$ContextCleaner$$keepCleaning(ContextCleaner.scala:189)\n",
      "\tat org.apache.spark.ContextCleaner$$anon$1.run(ContextCleaner.scala:79)\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o202.collectToPython.\n: org.apache.spark.SparkException: Job 26 cancelled because SparkContext was shut down\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cleanUpAfterSchedulerStop$1(DAGScheduler.scala:1188)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cleanUpAfterSchedulerStop$1$adapted(DAGScheduler.scala:1186)\n\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n\tat org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:1186)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:2887)\n\tat org.apache.spark.util.EventLoop.stop(EventLoop.scala:84)\n\tat org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:2784)\n\tat org.apache.spark.SparkContext.$anonfun$stop$11(SparkContext.scala:2105)\n\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1484)\n\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2105)\n\tat org.apache.spark.SparkContext$$anon$3.run(SparkContext.scala:2059)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_19461/1418833928.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mgroupBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'year'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcollect_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstruct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdf4\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstruct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"year\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"json_data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# O resultado é uma lista de objetos JSON\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DAA/lib/python3.7/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    815\u001b[0m         \"\"\"\n\u001b[1;32m    816\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 817\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectToPython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    818\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DAA/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1322\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DAA/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DAA/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o202.collectToPython.\n: org.apache.spark.SparkException: Job 26 cancelled because SparkContext was shut down\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cleanUpAfterSchedulerStop$1(DAGScheduler.scala:1188)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cleanUpAfterSchedulerStop$1$adapted(DAGScheduler.scala:1186)\n\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n\tat org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:1186)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:2887)\n\tat org.apache.spark.util.EventLoop.stop(EventLoop.scala:84)\n\tat org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:2784)\n\tat org.apache.spark.SparkContext.$anonfun$stop$11(SparkContext.scala:2105)\n\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1484)\n\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2105)\n\tat org.apache.spark.SparkContext$$anon$3.run(SparkContext.scala:2059)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "import json\n",
    "\n",
    "type(df4)\n",
    "\n",
    "\n",
    "# dataframe é o seu dataframe com as colunas mencionadas\n",
    "json_data = df4\\\n",
    "    .groupBy('year')\\\n",
    "    .agg(collect_list(struct(*df4.columns)).alias('data')) \\\n",
    "    .select(to_json(struct(col(\"year\"), col(\"data\"))).alias(\"json_data\")).collect()\n",
    "\n",
    "# O resultado é uma lista de objetos JSON\n",
    "json_list = [json.loads(row.json_data) for row in json_data]\n",
    "\n",
    "# Salva o resultado em um arquivo JSON\n",
    "with open('../parquetFiles2/output.json', 'w') as f:\n",
    "    json.dump(json_list, f)\n",
    "\n",
    "# Converte o resultado da consulta para um dicionário Python\n",
    "# result = spark\\\n",
    "#     .sql(\"SELECT year, collect_list(struct(quarter, group_name, group_value)) AS data FROM dados GROUP BY year\")\\\n",
    "#         .toJSON().map(json.loads).collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "acaffa62",
   "metadata": {},
   "source": [
    "----\n",
    "## Armazenar os dados no MongoDB"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "681ec660",
   "metadata": {},
   "source": [
    "#### **Teste para enviar dados para o MongoDB (na Organization que o Rodrigo Criou)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9bc6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "data = {'Nome': ['Maria', 'João', 'Pedro', 'Ana'],\n",
    "        'Idade': [25, 30, 20, 27],\n",
    "        'Cidade': ['São Paulo', 'Rio de Janeiro', 'Belo Horizonte', 'Curitiba']}\n",
    "\n",
    "dfTeste = pd.DataFrame(data)\n",
    "\n",
    "pqFile = dfTeste.to_parquet('../parquetFiles2/teste.parquet')\n",
    "pqFile = spark.read.format(\"parquet\").load('../parquetFiles2/teste.parquet')\n",
    "# pqFile = pqFile.write.parquet('../parquetFiles2/teste2.parquet')\n",
    "pqFile = spark.read.format(\"parquet\").load('../parquetFiles2/teste2.parquet')\n",
    "\n",
    "pqFile.show()\n",
    "\n",
    "pqFile.printSchema()\n",
    "\n",
    "pqFile.write.format(\"com.mongodb.spark.sql.DefaultSource\").option(\"database\", \"test\").option(\"collection\", \"inflation\").option(\"uri\", \"mongodb+srv://bigDataAdmin:admin@cluster0.of2q4ow.mongodb.net/test\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae18cf22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "\n",
    "df4.write.format(\"com.mongodb.spark.sql.DefaultSource\").option(\"database\", \"test\").option(\"collection\", \"inflation\").option(\"uri\", \"mongodb+srv://bigDataAdmin:admin@bigdatacluster.l1dei5j.mongodb.net/test\").mode(\"overwrite\").save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
